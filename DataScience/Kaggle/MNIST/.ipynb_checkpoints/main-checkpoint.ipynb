{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MYJZ9YODZIQx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import neural_network as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train.drop(['label'], axis=1), train[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yrSJV_VM2fe3"
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, stratify=train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = StandardScaler()\n",
    "train_x = np.array(mm.fit_transform(train_x))\n",
    "valid_x = np.array(mm.transform(valid_x))\n",
    "test = np.array(mm.transform(test))\n",
    "train_y = np.array(train_y)\n",
    "valid_y = np.array(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31500, 784), (10500, 784), (28000, 784))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, valid_x.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ecHyWgucZeZZ"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    EPSILON = 10**-8\n",
    "\n",
    "    def __init__(self, layers=[784,200,50,10], Weight_init='random', optimizer='gradient_descent', \n",
    "                 activation_function='tanh', Num_epochs=200, loss='cross_entropy', learning_rate=1e-5, \n",
    "                 Batch_size=64, Regularization=None, verbose=0, lambd=0.7, tol=1e-4, batch_norm=False):\n",
    "        weight_fn = {'random' : self.random, 'he':self.He, 'xavier':self.Xavier}\n",
    "        loss_fn = {'cross_entropy': self.cross_entropy_loss}\n",
    "        act_fn = {'relu': [self.relu, self.relu_grad], 'leakyrelu': [self.leakyrelu, self.leakyrelu_grad], 'tanh': [self.tanh, self.tanh_grad], 'sigmoid': [self.sigmoid, self.sigmoid_grad]}\n",
    "        opt_fn = {'gradient_descent': self.gradient_descent, 'momentum': self.gradient_descent_with_momentum,'nag':self.NAG, 'adagrad': self.Adagrad, 'rmsprop': self.RMSProp, 'adam': self.Adam}\n",
    "        reg_fn = {None: [lambda x: 0, lambda x, y: 0], 'L1': [self.L1, self.L1_grad], 'L2': [self.L2, self.L2_grad]}\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.weight_init = weight_fn[Weight_init]\n",
    "        self.is_gd = optimizer=='gradient_descent'\n",
    "        self.optimizer = opt_fn[optimizer]\n",
    "        self.act = act_fn[activation_function]\n",
    "        self.num_epochs = Num_epochs\n",
    "        self.loss = loss_fn[loss]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = Batch_size\n",
    "        self.regularization = reg_fn[Regularization]\n",
    "        self.verbose = verbose\n",
    "        self.lambd = lambd\n",
    "        self.tol = tol\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weights = [None]*(self.n_layers)\n",
    "        self.momentum = [None]*(self.n_layers)\n",
    "        self.G = [None]*(self.n_layers)\n",
    "        for i in range(1, self.n_layers):\n",
    "            # self.weights[i] = np.random.randn(self.layers[i], self.layers[i-1]) * 0.01\n",
    "            self.momentum[i] = np.zeros((self.layers[i], self.layers[i-1]))\n",
    "            self.G[i] = np.zeros((self.layers[i], self.layers[i-1]))\n",
    "            self.weights[i] = self.weight_init((self.layers[i], self.layers[i-1]), i)\n",
    "\n",
    "    def random(self, shape, l):\n",
    "        return np.random.normal(0, 1, shape) * 0.01\n",
    "\n",
    "    def He(self, shape, l):\n",
    "        return np.random.normal(0, np.sqrt(2/self.layers[l-1]), shape)\n",
    "\n",
    "    def Xavier(self, shape, l):\n",
    "        return np.random.normal(0, np.sqrt(1/self.layers[l-1]), shape)\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.where(Z > 0, Z, 0)\n",
    "    \n",
    "    def leakyrelu(self, Z):\n",
    "        return np.where(Z > 0, Z, Z * 0.01)\n",
    "\n",
    "    def tanh(self, Z):\n",
    "        return np.tanh(Z)\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        '''Numerically stable softmax\n",
    "        '''\n",
    "        exp = np.exp(Z - np.max(Z))\n",
    "        return np.true_divide(exp, exp.sum(axis=0, keepdims=True))\n",
    "\n",
    "    def relu_grad(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "    \n",
    "    def leakyrelu_grad(self, A):\n",
    "        return np.where(A > 0, 1, 0.01)\n",
    "    \n",
    "    def tanh_grad(self, A):\n",
    "        return 1 - A**2\n",
    "\n",
    "    def sigmoid_grad(self, A):\n",
    "        return A * (1 - A)\n",
    "\n",
    "    def gradient_descent(self, grads, epoch):\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * grads[i]\n",
    "\n",
    "    def gradient_descent_with_momentum(self, grads, epoch):\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.momentum[i] = 0.9 * self.momentum[i] - self.learning_rate * grads[i]\n",
    "            self.weights[i] = self.weights[i] + self.momentum[i]\n",
    "\n",
    "    def NAG(self, grads, epoch):\n",
    "        for i in range(1,self.n_layers):\n",
    "            x = 0.9*self.momentum[i]\n",
    "            self.momentum[i] = x - self.learning_rate * (grads[i] + x)\n",
    "            self.weights[i] = self.weights[i] + self.momentum[i]\n",
    "    \n",
    "    def Adagrad(self, grads, epoch):\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.G[i] +=  np.square(grads[i])\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * np.true_divide(grads[i], np.sqrt(self.G[i] + self.EPSILON))\n",
    "\n",
    "    def RMSProp(self, grads, epoch):\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.G[i] = 0.999 * self.G[i] + 0.001 * np.square(grads[i])\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * np.true_divide(grads[i], np.sqrt(self.G[i] + self.EPSILON))\n",
    "        \n",
    "    def Adam(self, grads, epoch):\n",
    "        for i in range(1,self.n_layers):\n",
    "            # self.momentum[i] = np.true_divide(0.9 * self.momentum[i] + 0.1*grads[i], 1-0.9**epoch)\n",
    "            # self.G[i] = np.true_divide(0.999 * self.G[i] + 0.001*np.square(grads[i]), 1-0.999**epoch)\n",
    "            self.momentum[i] = 0.9 * self.momentum[i] + 0.1*grads[i]\n",
    "            self.G[i] = 0.999 * self.G[i] + 0.001*np.square(grads[i])\n",
    "            self.weights[i] = self.weights[i] - (self.learning_rate * self.momentum[i] / (np.sqrt(self.G[i]) + self.EPSILON))\n",
    "\n",
    "    def L1(self, m):\n",
    "        sum = 0\n",
    "        for i in range(1, self.n_layers):\n",
    "            sum += np.sum(np.abs(self.weights[i]))\n",
    "        return self.lambd * sum / (2*m)\n",
    "\n",
    "    def L1_grad(self, m, i):\n",
    "        return np.where(self.weights[i] >= 0, 1, -1)\n",
    "\n",
    "    def L2(self, m):\n",
    "        sum = 0\n",
    "        for i in range(1, self.n_layers):\n",
    "            sum += np.sum(np.square(self.weights[i]))\n",
    "        return self.lambd * sum / (2*m)\n",
    "\n",
    "    def L2_grad(self, m, i):\n",
    "        return (self.lambd/m) * self.weights[i]\n",
    "\n",
    "    def cross_entropy_loss(self, AL, y):\n",
    "        loss = -np.mean((y*np.log(AL.T + self.EPSILON))) * self.num_classes + self.regularization[0](y.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def forward(self, X):\n",
    "        cache = [None]*(self.n_layers)\n",
    "        cache[0] = X.T\n",
    "        for i in range(1, self.n_layers):\n",
    "            Z = self.weights[i] @ cache[i-1]\n",
    "            if i != self.n_layers -1:\n",
    "                A = self.act[0](Z)\n",
    "            else:\n",
    "                A = self.softmax(Z)\n",
    "            cache[i] = A\n",
    "        return cache[self.n_layers-1], cache\n",
    "\n",
    "    def backward(self, cache, loss, y):\n",
    "        m = y.shape[0]\n",
    "        grads = {}\n",
    "        dZ = cache[self.n_layers-1] - y.T\n",
    "        dW = (1 / m) * dZ @ cache[self.n_layers-2].T\n",
    "        grads[self.n_layers-1] = dW\n",
    "        dA_prev = self.weights[self.n_layers-1].T @ dZ\n",
    "        for i in reversed(range(1, self.n_layers-1)):\n",
    "            dZ = dA_prev * self.act[1](cache[i])\n",
    "            dW = (1 / m) * (dZ @ cache[i-1].T) + self.regularization[1](m, i)\n",
    "            dA_prev = self.weights[i].T @ dZ\n",
    "            grads[i] = dW\n",
    "        return grads\n",
    "    \n",
    "    def make_batches(self, X, y):\n",
    "        if self.is_gd:\n",
    "            self.batch_size = X.shape[0]\n",
    "            \n",
    "        shuffle_indices = np.random.permutation(X.shape[0])\n",
    "        X = X[shuffle_indices]\n",
    "        y = y[shuffle_indices]\n",
    "        batches = []\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            if i + self.batch_size <= X.shape[0]:\n",
    "                xb, yb = X[i: i + self.batch_size], y[i: i + self.batch_size]\n",
    "                if self.batch_norm:\n",
    "                    xb = self.batch_normalization(xb)\n",
    "                batches.append((xb, yb))\n",
    "            else:\n",
    "                xb, yb = X[i:], y[i:]\n",
    "                if self.batch_norm:\n",
    "                    xb = self.batch_normalization(xb)\n",
    "                batches.append((xb, yb))\n",
    "        return batches\n",
    "\n",
    "    def batch_normalization(self, batch):\n",
    "        batch_mean = np.mean(batch, axis=0)\n",
    "        batch_var = np.var(batch, axis=0)\n",
    "        batch_norm = (batch - batch_mean) / np.sqrt(batch_var + self.EPSILON)\n",
    "        return batch_norm\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #Adding another input dimention for bias\n",
    "        X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        self.layers[0] += 1\n",
    "\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        y = self.one_hot_encode(self.num_classes, y)\n",
    "        if X_val is not None:\n",
    "            X_val = np.hstack((X_val, np.ones((X_val.shape[0], 1))))\n",
    "            y_val = self.one_hot_encode(self.num_classes,y_val)\n",
    "\n",
    "        batches = self.make_batches(X, y)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        stop = 0\n",
    "        epochs = self.num_epochs\n",
    "        if epochs == 'auto':\n",
    "            epochs = 200\n",
    "        for epoch in range(1, epochs+1):\n",
    "            for xb, yb in batches:\n",
    "                AL, cache = self.forward(xb)\n",
    "                loss = self.loss(AL, yb)\n",
    "                grads = self.backward(cache, loss, yb)\n",
    "                self.optimizer(grads, epoch)\n",
    "\n",
    "            loss = self.loss(self.forward(X)[0], y)\n",
    "            train_loss.append(loss)\n",
    "            if X_val is not None:\n",
    "                val_loss = self.loss(self.forward(X_val)[0], y_val)\n",
    "                valid_loss.append(val_loss)\n",
    "            \n",
    "            if self.verbose == 1 and epoch%1 == 0:\n",
    "                print(f'Epoch: {epoch}    Loss: {loss}', end='')\n",
    "                if X_val is not None:\n",
    "                    print(f'    Val Loss: {val_loss}', end='')\n",
    "                print()\n",
    "            if math.isnan(train_loss[-1]):\n",
    "                print('NaN loss')\n",
    "                break\n",
    "            if self.num_epochs == 'auto' and epoch > 1:\n",
    "                if stop == 10:\n",
    "                    self.num_epochs = epoch\n",
    "                    print(f'Training loss did not improve more than tol={self.tol} for 10 consecutive epochs. Stopping. (Iter={epoch})')\n",
    "                    break\n",
    "                elif train_loss[-2] - train_loss[-1] < self.tol:\n",
    "                    stop += 1\n",
    "                else:\n",
    "                    stop = 0\n",
    "        if self.num_epochs == 'auto':\n",
    "            self.num_epochs = 200\n",
    "            \n",
    "        if self.verbose == 1:\n",
    "            fig = plt.figure(figsize=(12,8))\n",
    "            plt.plot(range(self.num_epochs), train_loss, label='Training Loss')\n",
    "            plt.plot(range(self.num_epochs), valid_loss, label='Validation Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            return train_loss, valid_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if X.shape[1] < self.layers[0]:\n",
    "            X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        return self.forward(X)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot_encode(num_classes, y):\n",
    "        one_hot_encoded_y = np.eye(num_classes)[y.reshape(-1)]\n",
    "        return one_hot_encoded_y\n",
    "\n",
    "    def get_params(self):\n",
    "        return np.array(self.weights[1:])\n",
    "\n",
    "    def score(self, X, y):\n",
    "        '''X: a numpy array of shape (num_examples, num_features): This 2d matrix contains the\n",
    "        complete dataset.\n",
    "        Y: a numpy array of shape (num_examples): This array contains the classification labels\n",
    "        of the task.\n",
    "        Returns\n",
    "        -------\n",
    "        (float) Classification accuracy given X and y.\n",
    "        '''\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vn7VSmgBxdix",
    "outputId": "2ce2fdc7-7b05-439c-cfef-132db56eab43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1    Loss: 0.12944392929455398    Val Loss: 0.1982111453023027\n",
      "Epoch: 2    Loss: 0.08386447788507333    Val Loss: 0.16474022201232214\n",
      "Epoch: 3    Loss: 0.06230452769875499    Val Loss: 0.14807663259343057\n",
      "Epoch: 4    Loss: 0.04944200707536857    Val Loss: 0.13786651524042634\n",
      "Epoch: 5    Loss: 0.0407339821245642    Val Loss: 0.13367657902155944\n",
      "Epoch: 6    Loss: 0.034318818682150254    Val Loss: 0.12928350983432402\n",
      "Epoch: 7    Loss: 0.029487422833378657    Val Loss: 0.1256467285611801\n",
      "Epoch: 8    Loss: 0.025741413153360548    Val Loss: 0.12253430224486302\n",
      "Epoch: 9    Loss: 0.022750019536629935    Val Loss: 0.11992568013977302\n",
      "Epoch: 10    Loss: 0.02033275703837436    Val Loss: 0.11755112951690319\n",
      "Epoch: 11    Loss: 0.01841581403057561    Val Loss: 0.1154387552524915\n",
      "Epoch: 12    Loss: 0.016825328024738278    Val Loss: 0.11382274673319023\n",
      "Epoch: 13    Loss: 0.015493175040589115    Val Loss: 0.11225217013417654\n",
      "Epoch: 14    Loss: 0.014379985195012471    Val Loss: 0.11048437068696117\n",
      "Epoch: 15    Loss: 0.013421106743235937    Val Loss: 0.10961016397931206\n",
      "Epoch: 16    Loss: 0.012598634880853114    Val Loss: 0.10808838399615059\n",
      "Epoch: 17    Loss: 0.011902550534134049    Val Loss: 0.10726558025862368\n",
      "Epoch: 18    Loss: 0.011288508846534751    Val Loss: 0.10530479468916916\n",
      "Epoch: 19    Loss: 0.010739050675185473    Val Loss: 0.10526343082196975\n",
      "Epoch: 20    Loss: 0.010247050445469188    Val Loss: 0.10378272914409396\n",
      "Epoch: 21    Loss: 0.009826488725049194    Val Loss: 0.10352069883166233\n",
      "Epoch: 22    Loss: 0.009468998413696073    Val Loss: 0.10124683275524875\n",
      "Epoch: 23    Loss: 0.009069952081320845    Val Loss: 0.10203782143115761\n",
      "Epoch: 24    Loss: 0.008772955931735476    Val Loss: 0.09982497521047341\n",
      "Epoch: 25    Loss: 0.008461352772199029    Val Loss: 0.10075360299260935\n",
      "Epoch: 26    Loss: 0.008234903073523293    Val Loss: 0.09904408944971342\n",
      "Epoch: 27    Loss: 0.011063213191765047    Val Loss: 0.10341632243906435\n",
      "Epoch: 28    Loss: 0.009726242361796547    Val Loss: 0.10474145108317978\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlp = MLPClassifier(layers = [784, 200, 10], \n",
    "                    learning_rate = 5e-4,\n",
    "                    activation_function = 'relu', \n",
    "                    Num_epochs = 'auto', \n",
    "                    optimizer = 'adam',\n",
    "                    Weight_init = 'he',\n",
    "                    Batch_size = 256,\n",
    "                    verbose = 1,\n",
    "                    Regularization = 'L2',\n",
    "                    lambd = 0.1)\n",
    "mlp.fit(train_x, train_y, valid_x, valid_y)\n",
    "print(f'Training Accuracy: {mlp.score(train_x, train_y) * 100}%')\n",
    "print(f'Validation Accuracy: {mlp.score(valid_x, val_y) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QO509TsCQFy1",
    "outputId": "c96ccd0e-a02a-48de-c4dd-9c18d261e0ca"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9a766688b126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_X' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(val_X)\n",
    "print(classification_report(val_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "NhxglmX28oI3",
    "outputId": "2dda7547-ba67-4ef5-8739-5be7487f4db1"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_y, y_pred)\n",
    "plt.figure(figsize=(12,9))\n",
    "sns.heatmap(cm, linewidths=1, annot=True, fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "UpZHdk2AQh8Z",
    "outputId": "a703b6be-9563-4925-eaf4-2bc90fa06e79"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# optimizers = ['adam','rmsprop','adagrad','nag']\n",
    "# losses = {}\n",
    "# epochs = {}\n",
    "# for opt in optimizers:\n",
    "#     print(f'Training with {opt} optimizer')\n",
    "#     mlp = MLPClassifier(layers = [784, 200, 50, 10], \n",
    "#                         learning_rate = 1e-4,\n",
    "#                         activation_function = 'relu', \n",
    "#                         Num_epochs = 'auto', \n",
    "#                         optimizer = opt,\n",
    "#                         Weight_init = 'he',\n",
    "#                         Batch_size = 256,\n",
    "#                         verbose = 2,\n",
    "#                         Regularization = 'L2',\n",
    "#                         lambd = 0.9)\n",
    "#     losses[opt] = mlp.fit(train_X, train_y, val_X, val_y)[1]\n",
    "#     epochs[opt] = mlp.num_epochs\n",
    "#     print(f'Validation Accuracy: {mlp.score(val_X, val_y) * 100}%')\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "# for opt in optimizers:\n",
    "#     try:\n",
    "#         plt.plot(range(epochs[opt]), losses[opt], label=opt)\n",
    "#     except:\n",
    "#         print(f'Error in {opt} plot')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0VKpk7J89Q5",
    "outputId": "891df2c1-3510-435b-f4ac-4c94661cad44"
   },
   "outputs": [],
   "source": [
    "# skl_mlp = nn.MLPClassifier(hidden_layer_sizes=(256, 128, 64), learning_rate_init=1e-5, random_state=42, verbose=1)\n",
    "# skl_mlp.fit(train_X, train_y)\n",
    "# skl_mlp.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZuf-fs88y16"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL_Ass2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
